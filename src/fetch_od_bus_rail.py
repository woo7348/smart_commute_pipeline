"""
fetch_od_bus_rail.py
---------------------
ì¼ì¼ OD(ë²„ìŠ¤/ë„ì‹œì² ë„) ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.
NiFi ìë™í™”ì— ë§ê²Œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° ì²˜ë¦¬ ë°©ì‹ ë¦¬íŒ©í† ë§.

ë³€ê²½ ì‚¬í•­:
1. data/raw/od/YYYY/MM êµ¬ì¡°ë¡œ ì €ì¥ (NiFiê°€ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìµœì )
2. Safe Request ë„ì… (ì¬ì‹œë„/ì§€ì—°)
3. ì‚¬ìš©ì ì…ë ¥ ì œê±° (ìë™í™” í™˜ê²½ì— ë§ì¶¤)
4. ì—¬ëŸ¬ JSON â†’ ì›” ë‹¨ìœ„ ë³‘í•© â†’ ì¤‘ê°„íŒŒì¼ ì‚­ì œ
5. ì ˆëŒ€ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ê²½ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬

Author : MinWoo Kang
Project : Smart Commute Pipeline
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from dotenv import load_dotenv


# --------------------------------------------------------------------
# ğŸŒ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# --------------------------------------------------------------------
load_dotenv()
SERVICE_KEY = os.getenv("OD_API_KEY")
if not SERVICE_KEY:
    raise ValueError("âŒ OD_API_KEY not found in .env file")

BASE_URL = (
    "https://apis.data.go.kr/1613000/"
    "ODUsageforGeneralBusesandUrbanRailways/getDailyODUsageforGeneralBusesandUrbanRailways"
)

# ì ˆëŒ€ ê²½ë¡œ êµ¬ì„±
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RAW_OD_DIR = os.path.join(BASE_DIR, "../data/raw/od")


# --------------------------------------------------------------------
# ğŸ“ JSON ì €ì¥ í•¨ìˆ˜ (NiFi Friendly)
# --------------------------------------------------------------------
def save_json(year, month, filename, data):
    """
    data/raw/od/YYYY/MM/filename.json í˜•ì‹ìœ¼ë¡œ ì €ì¥
    """
    year_dir = os.path.join(RAW_OD_DIR, str(year))
    month_dir = os.path.join(year_dir, f"{month:02d}")

    os.makedirs(month_dir, exist_ok=True)

    filepath = os.path.join(month_dir, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ Saved â†’ {filepath}")
    return filepath


# --------------------------------------------------------------------
# ğŸ” Safe Request (ì¬ì‹œë„ í¬í•¨)
# --------------------------------------------------------------------
def safe_request(params, max_retries=3, delay=5):
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(BASE_URL, params=params, timeout=(5, 60))
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"âš ï¸ Attempt {attempt}/{max_retries} failed: {e}")
            time.sleep(delay)

    print("âŒ All retry attempts failed.")
    return None


# --------------------------------------------------------------------
# ğŸ“¥ ì¼ìë³„ API í˜¸ì¶œ
# --------------------------------------------------------------------
def fetch_od_daily(opr_ymd, year, month,
                   dptre_ctpv_cd="11", dptre_sgg_cd="11110",
                   arvl_ctpv_cd="11", arvl_sgg_cd="11680"):
    """
    ì¼ìë³„ OD ë°ì´í„° í˜¸ì¶œ í›„ JSON ì €ì¥
    """

    params = {
        "serviceKey": SERVICE_KEY,
        "pageNo": "1",
        "numOfRows": "100",
        "opr_ymd": opr_ymd,
        "dptre_ctpv_cd": dptre_ctpv_cd,
        "dptre_sgg_cd": dptre_sgg_cd,
        "arvl_ctpv_cd": arvl_ctpv_cd,
        "arvl_sgg_cd": arvl_sgg_cd,
        "dataType": "JSON",
    }

    print(f"ğŸ“¡ Requesting OD: {opr_ymd}")

    response = safe_request(params)
    if not response:
        return None

    try:
        data = response.json()
    except Exception:
        print(f"âŒ JSON Parse Error on {opr_ymd}")
        return None

    # ë‚´ë¶€ êµ¬ì¡° íƒìƒ‰
    items = data.get("Response", {}).get("body", {}).get("items", {}).get("item", [])

    filename = f"od_{opr_ymd}.json"
    return save_json(year, month, filename, items)


# --------------------------------------------------------------------
# ğŸ§© ì—¬ëŸ¬ JSON ë³‘í•©
# --------------------------------------------------------------------
def merge_json_files(file_list, year, month, output_filename):
    """ì—¬ëŸ¬ JSON íŒŒì¼ ë³‘í•©"""

    all_items = []

    for file_path in file_list:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    all_items.extend(data)
        except Exception as e:
            print(f"âš ï¸ Skipped {file_path}: {e}")

    # ì €ì¥
    return save_json(year, month, output_filename, all_items)


# --------------------------------------------------------------------
# ğŸŒ• ì›” ì „ì²´ ìˆ˜ì§‘ (ì¼ë³„ ìš”ì²­ â†’ ë³‘í•© â†’ ì¤‘ê°„ì‚­ì œ)
# --------------------------------------------------------------------
def fetch_od_monthly(year, month, chunk_size=10, **kwargs):
    """
    1. í•˜ë£¨ ë‹¨ìœ„ë¡œ API ìš”ì²­
    2. chunk_size(10ì¼) ë‹¨ìœ„ë¡œ ë³‘í•©
    3. ë§ˆì§€ë§‰ì— full íŒŒì¼ ìƒì„±
    """

    print(f"ğŸ“… Fetching OD Monthly â†’ {year}-{month:02d}")

    start_date = datetime(year, month, 1)
    end_date = (
        datetime(year + (month // 12), (month % 12) + 1, 1)
        - timedelta(days=1)
    )

    current = start_date
    temp_files = []
    chunk_files = []

    chunk_start_date = current.strftime("%Y%m%d")

    while current <= end_date:
        opr_ymd = current.strftime("%Y%m%d")

        file_path = fetch_od_daily(opr_ymd, year, month, **kwargs)
        if file_path:
            temp_files.append(file_path)

        # 10ì¼ ë‹¨ìœ„ ë³‘í•©
        if len(temp_files) == chunk_size or current == end_date:
            chunk_end_date = opr_ymd
            chunk_filename = f"od_{chunk_start_date}_to_{chunk_end_date}.json"

            merged_path = merge_json_files(temp_files, year, month, chunk_filename)
            chunk_files.append(merged_path)

            # temp íŒŒì¼ ì‚­ì œ
            for tmp in temp_files:
                try:
                    os.remove(tmp)
                except:
                    pass
            temp_files = []

            # ë‹¤ìŒ chunk ì‹œì‘ì  ê°±ì‹ 
            if current < end_date:
                chunk_start_date = (current + timedelta(days=1)).strftime("%Y%m%d")

        current += timedelta(days=1)
        time.sleep(1)

    # ì›” ì „ì²´ ë³‘í•©
    final_filename = f"od_{year}{month:02d}_all.json"
    final_path = merge_json_files(chunk_files, year, month, final_filename)

    # ì¤‘ê°„ chunk ì‚­ì œ
    for c in chunk_files:
        try:
            os.remove(c)
        except:
            pass

    print(f"ğŸŒ• Final merged file: {final_path}")
    return final_path


# --------------------------------------------------------------------
# ğŸš€ ì‹¤í–‰ë¶€ (ìë™í™” + ëˆ„ë½ ì›” ë°±í•„)
# --------------------------------------------------------------------
if __name__ == "__main__":
    start_year = 2025
    start_month = 4

    today = datetime.now()
    current_year = today.year
    current_month = today.month

    # ëˆ„ë½ ì›” ìë™ ìˆ˜ì§‘
    for year in range(start_year, current_year + 1):

        # ì‹œì‘ ì›” ì„¤ì •
        if year == start_year:
            m_start = start_month
        else:
            m_start = 1

        # ê²€ì‚¬í•  ë§ˆì§€ë§‰ ì›”
        if year < current_year:
            m_end = 12
        else:
            m_end = current_month - 1

        if m_end < 1:
            continue

        for month in range(m_start, m_end + 1):
            expected = os.path.join(
                RAW_OD_DIR,
                f"{year}/{month:02d}/od_{year}{month:02d}_all.json"
            )

            if not os.path.exists(expected):
                print(f"ğŸ“¡ Missing â†’ Fetching {year}-{month:02d}")
                fetch_od_monthly(year, month)
            else:
                print(f"âœ… Exists: {expected}")

    print("ğŸ‰ OD Extract Completed Successfully")
