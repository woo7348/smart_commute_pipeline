"""
fetch_weather.py
-----------------
ê¸°ìƒì²­ ASOS(ì¢…ê´€ê¸°ìƒê´€ì¸¡) ì¼ë³„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.
- í•œ ë²ˆì— í•œ ë‹¬ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë˜, ì„œë²„ íƒ€ì„ì•„ì›ƒ(504 ì˜¤ë¥˜)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´
  ì£¼ ë‹¨ìœ„(7ì¼ ë‹¨ìœ„)ë¡œ ë‚˜ëˆ ì„œ APIë¥¼ ë°˜ë³µ í˜¸ì¶œ.
- ê° ì£¼ê°„ ë°ì´í„°ë¥¼ raw í´ë”ì— ê°œë³„ JSONìœ¼ë¡œ ì €ì¥í•œ ë’¤, ìµœì¢…ì ìœ¼ë¡œ í•œ ë‹¬ì¹˜ ë°ì´í„°ë¥¼ ë³‘í•©.

Author : MinWoo Kang
Project: Smart Commute Pipeline
"""

import os
import requests
import json
import time
from dotenv import load_dotenv
from datetime import datetime, timedelta

# --------------------------------------------------------------------
# âœ… í™˜ê²½ ë³€ìˆ˜ ë° API ê¸°ë³¸ ì„¤ì •
# --------------------------------------------------------------------
load_dotenv()
SERVICE_KEY = os.getenv("WEATHER_API_KEY")
if not SERVICE_KEY:
    raise ValueError("âŒ WEATHER_API_KEY not found in .env file")

BASE_URL = "http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList"

# --------------------------------------------------------------------
# âœ… 1ï¸âƒ£ ì•ˆì „í•œ ìš”ì²­ í•¨ìˆ˜ (ì¬ì‹œë„ + ì§€ì—° í¬í•¨)
# --------------------------------------------------------------------
def safe_request(params, max_retries=3, delay=5):
    """ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ + ì§€ì—°"""
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(BASE_URL, params=params, timeout=(5, 60))
            response.raise_for_status()
            return response
        except requests.exceptions.Timeout:
            print(f"â³ Timeout on attempt {attempt}/{max_retries}. Retrying in {delay}s...")
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Request failed on attempt {attempt}/{max_retries}: {e}")
        time.sleep(delay)
    print("âŒ All retry attempts failed.")
    return None


# --------------------------------------------------------------------
# âœ… 2ï¸âƒ£ ë‹¨ì¼ êµ¬ê°„ ë°ì´í„° ìš”ì²­ í•¨ìˆ˜
# --------------------------------------------------------------------
def fetch_asos_daily(start_date, end_date, stn_id="108"):
    """ê¸°ìƒì²­ ASOS ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ì£¼ê°„ ë‹¨ìœ„)"""
    filename = f"asos_daily_{stn_id}_{start_date}_{end_date}.json"
    params = {
        "serviceKey": SERVICE_KEY,
        "dataCd": "ASOS",
        "dateCd": "DAY",
        "startDt": start_date,
        "endDt": end_date,
        "stnIds": stn_id,
        "dataType": "JSON",
        "numOfRows": 100,
        "pageNo": 1,
    }

    print(f"ğŸ“¡ Requesting ASOS Daily data ({start_date} ~ {end_date}, stn={stn_id})")
    response = safe_request(params)

    if not response:
        print(f"âŒ Skipping period ({start_date} ~ {end_date}) due to repeated failures.")
        return None

    data = response.json()
    items = data.get("response", {}).get("body", {}).get("items", {}).get("item", [])

    if not items:
        print("âš ï¸ No data found for given date range or station ID.")
        return None

    os.makedirs("raw", exist_ok=True)
    filepath = f"raw/{filename}"

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved ASOS daily data â†’ {filepath}")
    return items


# --------------------------------------------------------------------
# âœ… 3ï¸âƒ£ ì›” ë‹¨ìœ„ ë°ì´í„° ë³‘í•© í•¨ìˆ˜ (ì£¼ ë‹¨ìœ„ ë¶„í•  í˜¸ì¶œ)
# --------------------------------------------------------------------
def fetch_asos_month_chunked(year=2025, month=5, stn_id="108"):
    """í•œ ë‹¬ ë°ì´í„°ë¥¼ ì£¼ ë‹¨ìœ„ë¡œ ë¶„í•  ìš”ì²­ í›„ ë³‘í•© ì €ì¥ + ì¤‘ê°„ íŒŒì¼ ìë™ ì‚­ì œ"""
    start = datetime(year, month, 1)
    if month == 12:
        end = datetime(year + 1, 1, 1) - timedelta(days=1)
    else:
        end = datetime(year, month + 1, 1) - timedelta(days=1)

    all_items = []             # ì „ì²´ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    saved_files = []           # âœ… ì—¬ê¸°ì„œ ë°˜ë“œì‹œ ì´ˆê¸°í™” (ì¤‘ê°„íŒŒì¼ ê²½ë¡œ ì €ì¥ìš©)
    delta = timedelta(days=7)  # 7ì¼ ê°„ê²©

    while start <= end:
        chunk_start = start.strftime("%Y%m%d")
        chunk_end = min(start + delta - timedelta(days=1), end).strftime("%Y%m%d")

        print(f"ğŸ“… Fetching chunk: {chunk_start} ~ {chunk_end}")
        items = fetch_asos_daily(chunk_start, chunk_end, stn_id)

        if items:
            all_items.extend(items)
            filename = f"raw/asos_daily_{stn_id}_{chunk_start}_{chunk_end}.json"
            saved_files.append(filename)  # âœ… íŒŒì¼ ê²½ë¡œ ëˆ„ì  ì €ì¥

        time.sleep(2)
        start += delta

    # âœ… ë³‘í•© ë° ì¤‘ê°„íŒŒì¼ ì‚­ì œ
    if all_items:
        os.makedirs("raw", exist_ok=True)
        merged_path = f"raw/asos_daily_{stn_id}_{year}{month:02d}_full.json"

        with open(merged_path, "w", encoding="utf-8") as f:
            json.dump(all_items, f, ensure_ascii=False, indent=2)

        print(f"âœ… Merged {len(all_items)} records â†’ {merged_path}")

        # ğŸ§¹ ì£¼ê°„ íŒŒì¼ ìë™ ì‚­ì œ
        for fpath in saved_files:
            if os.path.exists(fpath):
                os.remove(fpath)
                print(f"ğŸ—‘ï¸ Removed intermediate file â†’ {fpath}")
            else:
                print(f"âš ï¸ Skipped (not found): {fpath}")
    else:
        print("âš ï¸ No data collected for the month.")


# --------------------------------------------------------------------
# âœ… 4ï¸âƒ£ ì‹¤í–‰ë¶€
# --------------------------------------------------------------------
if __name__ == "__main__":
    fetch_asos_month_chunked(year=2025, month=5, stn_id="108")
